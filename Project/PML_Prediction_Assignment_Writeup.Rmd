---
title: "Practical Machine Learning: Prediction Assignment Writeup"
author: "Paul Foellbach"
date: "Thursday, June 11, 2015"
output: html_document
---
### Executive Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har. The goal of this project is to predict the manner in which they did the execise. The dataset used in this project can be found here, and the experiment details are described in the original paper.

For the assignment I analyzed the provided data to determine what activity an individual perform.
To realize this I made use of the R package caret and randomForest, this allowed me to build correct answers for
each of the 20 test data cases provided in this assignment.  I made use of a seed value for 
consistent results.

### Loading packages, loading and processing Data
```{r,message=FALSE, warning=FALSE}
library(Hmisc)
library(caret)
library(randomForest)
library(foreach)
library(doParallel)
set.seed(2048)
options(warn=-1)
```

First, I loaded the data both from the provided training and test data provided by COURSERA.
Some values contained a "#DIV/0!" that I replaced with an NA value.

```{r}
training_data <- read.csv("pml-training.csv", na.strings=c("#DIV/0!") )
evaluation_data <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!") )
```

I also casted all columns 8 to the end to be numeric.

```{r}
for(i in c(8:ncol(training_data)-1)) {training_data[,i] = as.numeric(as.character(training_data[,i]))}

for(i in c(8:ncol(evaluation_data)-1)) {evaluation_data[,i] = as.numeric(as.character(evaluation_data[,i]))}
```

Some columns were mostly blank. These did not contribute well to the prediction. I chose a feature
set that only included complete columns. I also remove user name, timestamps and windows.  

Determine and display out feature set.

```{r, results='hide'}
feature_set <- colnames(training_data[colSums(is.na(training_data)) == 0])[-(1:7)]
model_data <- training_data[feature_set]
feature_set
```

We now have the model data built from our feature set.
The data set is partioned in two parts. With the first part only the model parameters are estimated and on the basis of the second part the model error is calculated (out-of-sample error). The generalization of this procedure are the cross validation methods.

```{r}
idx <- createDataPartition(y=model_data$classe, p=0.75, list=FALSE )
training <- model_data[idx,]
testing <- model_data[-idx,]
```
      
### Modeling 5 random forests
We now build 5 random forests with 150 trees each. We make use of parallel processing to build this
model. I found different examples of how to do parallel processing with random forests in R, this
provided a big increase.

```{r}
registerDoParallel()
x <- training[-ncol(training)]
y <- training$classe

rf <- foreach(ntree=rep(150, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x, y, ntree=ntree) 
}
```

Provide error reports for both training and test data.

```{r, results='hide'}
predictions1 <- predict(rf, newdata=training)
confusionMatrix(predictions1,training$classe)
```
```{r}
predictions2 <- predict(rf, newdata=testing)
confusionMatrix(predictions2,testing$classe)
```

### Conclusions and Test Data Submit

As can be seen from the confusion matrix this model is very accurate. I did experiment with PCA 
and other models, but did not get a comparable accuracy. Because my test data was around 99% 
accurate (out of sample) I expected nearly all of the submitted test cases to be correct.  It turned out they 
were all correct.

Prepare the submission (using the provided code by Coursera).

```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}


x <- evaluation_data
x <- x[feature_set[feature_set!='classe']]
answers <- predict(rf, newdata=x)

answers

pml_write_files(answers)
```
